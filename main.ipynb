{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from focal_loss import FocalLoss\n",
    "from mock_dataset import MockOutfitDataset\n",
    "from outfit_model import OutfitCompatibilityModel\n",
    "import torch.nn as nn\n",
    "from utils import save_checkpoint\n",
    "\n",
    "# Instantiate the mock dataset and dataloader\n",
    "# Contains list of all outfits\n",
    "mock_dataset = MockOutfitDataset()\n",
    "\n",
    "# Organizes your dataset into batches.\n",
    "# Batch size = number of samples processed in one iteration\n",
    "# Number of batches = total samples divided by batch_size\n",
    "# Each this case, a sample = an outfit\n",
    "dataloader = torch.utils.data.DataLoader(mock_dataset, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "def show_images(images, labels):\n",
    "    for i in range(images.size(0)):\n",
    "        image = F.to_pil_image(images[i])\n",
    "        plt.subplot(1, images.size(0), i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Label: {labels[i]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for batch_idx, (images, texts, labels) in enumerate(dataloader):\n",
    "    print(\n",
    "        f\"Batch {batch_idx + 1} - Shape of images: {images.shape}, Texts: {texts}, Labels: {labels}\"\n",
    "    )\n",
    "\n",
    "    # Visualize the images\n",
    "    show_images(images, labels)\n",
    "    \n",
    "    if batch_idx == 2:  # Print information for the first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, dataset, and dataloader\n",
    "model = OutfitCompatibilityModel()\n",
    "focal_loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_image: tensor([[[6.1862e-01, 3.5895e-01, 1.3058e-01,  ..., 1.5398e-01,\n",
      "          1.7951e-01, 7.4827e-01],\n",
      "         [2.9103e-01, 5.4234e-01, 1.2398e-01,  ..., 3.4051e-01,\n",
      "          2.8241e-01, 8.4956e-01],\n",
      "         [5.9702e-01, 6.4334e-01, 2.3772e-01,  ..., 8.0319e-01,\n",
      "          4.2572e-01, 6.9479e-01],\n",
      "         ...,\n",
      "         [1.4980e-01, 6.9386e-01, 7.9831e-01,  ..., 2.4417e-01,\n",
      "          6.3358e-01, 7.4379e-01],\n",
      "         [7.7392e-01, 9.1118e-01, 4.0588e-01,  ..., 4.3054e-01,\n",
      "          4.0429e-01, 4.9255e-01],\n",
      "         [7.0293e-01, 5.3127e-01, 9.9506e-01,  ..., 1.3217e-02,\n",
      "          2.3647e-01, 7.1585e-05]],\n",
      "\n",
      "        [[8.3040e-01, 3.1672e-01, 3.8315e-01,  ..., 8.1234e-03,\n",
      "          5.4081e-01, 9.8516e-02],\n",
      "         [2.3183e-01, 5.5794e-01, 5.3545e-01,  ..., 6.1714e-01,\n",
      "          6.8902e-01, 5.4077e-01],\n",
      "         [4.2142e-01, 8.7561e-02, 5.2564e-01,  ..., 6.4588e-01,\n",
      "          8.3411e-01, 9.5979e-02],\n",
      "         ...,\n",
      "         [7.8803e-01, 4.4939e-01, 8.3936e-02,  ..., 7.3855e-01,\n",
      "          7.6017e-01, 3.1808e-01],\n",
      "         [8.8402e-01, 4.9678e-01, 7.5158e-01,  ..., 5.0110e-01,\n",
      "          4.0721e-02, 5.6767e-01],\n",
      "         [8.8339e-01, 7.4256e-01, 2.5332e-01,  ..., 3.6445e-01,\n",
      "          2.9657e-01, 5.9270e-01]],\n",
      "\n",
      "        [[2.8975e-01, 6.3463e-01, 4.6260e-01,  ..., 5.9140e-01,\n",
      "          2.0443e-02, 5.6903e-01],\n",
      "         [8.4169e-01, 7.0240e-01, 7.3845e-01,  ..., 2.7399e-01,\n",
      "          5.0034e-01, 8.4465e-01],\n",
      "         [6.8771e-01, 5.1394e-01, 7.1179e-01,  ..., 3.5498e-01,\n",
      "          7.2346e-01, 2.9771e-01],\n",
      "         ...,\n",
      "         [6.6217e-01, 3.3409e-02, 9.7024e-01,  ..., 7.6946e-01,\n",
      "          8.3346e-01, 4.3475e-01],\n",
      "         [9.9763e-01, 9.3682e-01, 3.8900e-01,  ..., 7.0944e-01,\n",
      "          7.6435e-01, 2.4034e-01],\n",
      "         [9.9431e-01, 6.2248e-01, 5.4039e-01,  ..., 7.4860e-01,\n",
      "          6.5125e-01, 6.9275e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3882e-01, 4.9829e-01, 2.0967e-01,  ..., 8.3473e-01,\n",
      "          1.2888e-01, 9.9494e-01],\n",
      "         [8.1655e-01, 8.9637e-01, 8.0153e-01,  ..., 5.3336e-01,\n",
      "          4.5902e-01, 1.1827e-01],\n",
      "         [5.7989e-01, 7.9267e-01, 5.8437e-02,  ..., 7.7453e-01,\n",
      "          6.3747e-01, 9.0376e-01],\n",
      "         ...,\n",
      "         [6.2842e-01, 5.4202e-01, 3.4130e-01,  ..., 4.7115e-01,\n",
      "          3.4131e-02, 6.2900e-01],\n",
      "         [8.8306e-01, 9.3854e-01, 3.8289e-01,  ..., 8.5032e-01,\n",
      "          5.3354e-01, 5.0236e-01],\n",
      "         [6.7667e-01, 7.3756e-01, 7.5949e-01,  ..., 1.5287e-01,\n",
      "          5.7403e-02, 5.6447e-01]],\n",
      "\n",
      "        [[1.0646e-01, 7.2750e-01, 7.7217e-01,  ..., 2.2851e-01,\n",
      "          7.0610e-02, 3.6151e-01],\n",
      "         [9.7023e-01, 7.9396e-01, 7.2993e-01,  ..., 7.5703e-01,\n",
      "          1.7881e-01, 3.1302e-01],\n",
      "         [7.0591e-01, 2.3337e-01, 1.1704e-01,  ..., 4.4583e-01,\n",
      "          3.0373e-01, 8.5361e-01],\n",
      "         ...,\n",
      "         [5.2463e-01, 5.6069e-01, 3.0087e-02,  ..., 7.4706e-01,\n",
      "          9.2503e-01, 7.4409e-01],\n",
      "         [6.6869e-01, 7.7348e-01, 7.1562e-01,  ..., 8.2441e-01,\n",
      "          8.7916e-01, 2.3141e-01],\n",
      "         [8.8818e-01, 3.7261e-02, 2.0494e-01,  ..., 7.0784e-01,\n",
      "          4.9779e-01, 8.9892e-01]],\n",
      "\n",
      "        [[9.9636e-01, 6.4970e-01, 6.7100e-01,  ..., 7.7394e-01,\n",
      "          7.4727e-01, 4.4222e-01],\n",
      "         [5.4194e-01, 6.1045e-01, 6.4408e-01,  ..., 7.8217e-02,\n",
      "          5.8897e-01, 8.6832e-01],\n",
      "         [1.6305e-01, 4.6668e-01, 8.5627e-01,  ..., 6.5031e-01,\n",
      "          7.2480e-01, 6.5236e-01],\n",
      "         ...,\n",
      "         [9.8537e-01, 7.5483e-01, 7.9262e-02,  ..., 2.1530e-01,\n",
      "          6.4666e-01, 6.8629e-01],\n",
      "         [6.3113e-01, 6.4703e-01, 5.9509e-02,  ..., 5.0152e-01,\n",
      "          5.8832e-01, 6.1342e-01],\n",
      "         [6.2716e-01, 8.0362e-02, 9.4455e-01,  ..., 2.8689e-01,\n",
      "          5.2346e-01, 5.7528e-02]]]) item_text: mock description 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 20, 224, 224] to have 3 channels, but got 20 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\phamm\\MachineLearning_Playground\\outfit_recommendation\\main.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m images, texts, labels \u001b[39m=\u001b[39m batch  \u001b[39m# Adjust this based on your dataset structure\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images, texts)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m focal_loss(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs, labels\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )  \u001b[39m# Ensure labels have the right dimension\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phamm\\MachineLearning_Playground\\outfit_recommendation\\outfit_model.py:45\u001b[0m, in \u001b[0;36mOutfitCompatibilityModel.forward\u001b[1;34m(self, images, texts)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mitem_image: \u001b[39m\u001b[39m{\u001b[39;00mitem_image\u001b[39m}\u001b[39;00m\u001b[39m item_text: \u001b[39m\u001b[39m{\u001b[39;00mitem_text\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[39m# Encode item's image using ResNet18\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m item_image_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_encoder(item_image)\n\u001b[0;32m     47\u001b[0m \u001b[39m# Encode item's text using TextEncoder\u001b[39;00m\n\u001b[0;32m     48\u001b[0m item_text_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder(item_text)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phamm\\MachineLearning_Playground\\outfit_recommendation\\image_encoder.py:17\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet18(x)\n\u001b[0;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_layer(x)\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 20, 224, 224] to have 3 channels, but got 20 channels instead"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        images, texts, labels = batch  # Adjust this based on your dataset structure\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = focal_loss(\n",
    "            outputs, labels.unsqueeze(1)\n",
    "        )  # Ensure labels have the right dimension\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or log the loss if needed\n",
    "        print(f\"Epoch {epoch + 1}, Batch loss: {loss.item()}\")\n",
    "\n",
    "    save_checkpoint(model.state_dict(),\"mock\",f\"model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    # Adjust the learning rate as needed (reduce by half in steps of 10)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = param_group[\"lr\"] / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outfit_recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
