{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num_outfits of splits:train: 10000; valid: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from mock_dataset import MockOutfitDataset\n",
    "from outfit_model import OutfitCompatibilityModel\n",
    "from outfit_dataset import OutfitDataset\n",
    "import torch.nn as nn\n",
    "from utils import save_checkpoint\n",
    "import logging\n",
    "\n",
    "# DEBUG - INFO - WARNING - ERROR\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Should be disjoint/nondisjoint\n",
    "polyvore_split = \"nondisjoint\"\n",
    "\n",
    "# Should be traim/valid/test\n",
    "split_valid = \"valid\"\n",
    "split_train = \"train\"\n",
    "split_test = \"test\"\n",
    "\n",
    "default_batch_size = 25\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Organizes your dataset into batches. If we don't do this, our texts matrix will be transposed\n",
    "# Batch size = number of samples processed in one iteration\n",
    "# Number of batches = total samples divided by batch_size\n",
    "# Each this case, a sample = an outfit\n",
    "def custom_collate(batch):\n",
    "    outfits_images = []\n",
    "    outfits_texts = []\n",
    "    outfits_labels = []\n",
    "\n",
    "    # Find the maximum number of items in any outfit in this batch\n",
    "    max_items = max(len(outfit[\"outfit_images\"]) for outfit in batch)\n",
    "\n",
    "    for outfit in batch:\n",
    "        # Pad or truncate the number of items to match max_items\n",
    "        padded_images = torch.zeros((max_items,) + outfit[\"outfit_images\"].shape[1:])\n",
    "        padded_images[: outfit[\"outfit_images\"].shape[0]] = outfit[\"outfit_images\"]\n",
    "\n",
    "        # Similarly, pad or truncate the number of texts\n",
    "        padded_texts = outfit[\"outfit_texts\"] + [\"\"] * (\n",
    "            max_items - len(outfit[\"outfit_texts\"])\n",
    "        )\n",
    "\n",
    "        outfits_images.append(padded_images)\n",
    "        outfits_texts.append(padded_texts)\n",
    "        outfits_labels.append(outfit[\"outfit_label\"])\n",
    "\n",
    "    return {\n",
    "        \"outfit_images\": torch.stack(outfits_images),\n",
    "        \"outfit_texts\": outfits_texts,\n",
    "        \"outfit_labels\": torch.tensor(outfits_labels, dtype=torch.float),\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = OutfitDataset(data_dir, polyvore_split, split_train, transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=default_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate,\n",
    ")\n",
    "\n",
    "valid_dataset = OutfitDataset(data_dir, polyvore_split, split_valid, transform)\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=default_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate,\n",
    ")\n",
    "\n",
    "total_train_num_outfits = len(train_dataloader.dataset)\n",
    "total_valid_num_outfits = len(valid_dataloader.dataset)\n",
    "\n",
    "print(\n",
    "    f\"total_num_outfits of splits:{split_train}: {total_train_num_outfits}; {split_valid}: {total_valid_num_outfits}\"\n",
    ")\n",
    "\n",
    "# Instantiate the mock dataset and dataloader\n",
    "# Contains list of all outfits\n",
    "# mock_dataset = MockOutfitDataset()\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     mock_dataset, batch_size=15, shuffle=True, collate_fn=custom_collate\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NOT USABLE\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# def show_images(images, labels):\n",
    "#     for i in range(images.size(1)):\n",
    "#         image = F.to_pil_image(images[:, i, ...])\n",
    "#         plt.subplot(1, images.size(1), i + 1)\n",
    "#         plt.imshow(image)\n",
    "#         plt.title(f\"Label: {labels[i]}\")\n",
    "#         plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# for batch_idx, (images, texts, labels) in enumerate(dataloader):\n",
    "#     print(\n",
    "#         f\"Batch {batch_idx + 1} - Shape of images: {images.shape}, Texts: {texts}, Labels: {labels}\"\n",
    "#     )\n",
    "\n",
    "#     # Visualize the images\n",
    "#     show_images(images, labels)\n",
    "    \n",
    "#     if batch_idx == 2:  # Print information for the first 3 batches\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and components for training (loss function, optimizer)\n",
    "model = OutfitCompatibilityModel()\n",
    "focal_loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def show_plots(plots, label):\n",
    "    for plot in plots:\n",
    "        plt.plot(plot[\"values\"], label=plot[\"label\"])\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test(epoch, dataloader, losses, auc_scores):\n",
    "    model.eval()\n",
    "    all_val_labels = []\n",
    "    all_val_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in dataloader:\n",
    "            val_images = val_batch[\"outfit_images\"]\n",
    "            val_texts = val_batch[\"outfit_texts\"]\n",
    "            val_labels = val_batch[\"outfit_labels\"]\n",
    "\n",
    "            val_outputs = model(val_images, val_texts)\n",
    "            val_loss = focal_loss(val_outputs, val_labels)\n",
    "\n",
    "            all_val_labels.extend(val_labels.cpu().numpy())\n",
    "            all_val_outputs.extend(val_outputs.cpu().numpy())\n",
    "\n",
    "    losses.append(val_loss.item() / len(dataloader))\n",
    "\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(all_val_labels, all_val_outputs)\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Epoch {epoch + 1}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "train_losses = []  # to store training losses\n",
    "val_losses = []  # to store validation losses\n",
    "val_auc_scores = []  # to store AUC scores\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        images = batch[\"outfit_images\"]\n",
    "        texts = batch[\"outfit_texts\"]\n",
    "        labels = batch[\"outfit_labels\"]\n",
    "\n",
    "        logging.debug(f\"batch - images.shape: {images.shape}\")\n",
    "        logging.debug(f\"batch - texts: {texts}\")\n",
    "        logging.debug(f\"batch - labels: {labels}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = focal_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    train_losses.append(loss.item() / len(train_dataloader))\n",
    "\n",
    "    # region Validation Phase\n",
    "    test(\n",
    "        epoch=epoch,\n",
    "        dataloader=valid_dataloader,\n",
    "        losses=val_losses,\n",
    "        auc_scores=val_auc_scores,\n",
    "    )\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    save_checkpoint(model.state_dict(), polyvore_split, f\"model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    # Adjust the learning rate as needed (reduce by half in steps of 10)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = param_group[\"lr\"] / 2\n",
    "\n",
    "show_plots(\n",
    "    plots=[\n",
    "        {\"values\": train_losses, \"label\": \"Train Losses\"},\n",
    "        {\"values\": val_losses, \"label\": \"Validation Losses\"},\n",
    "    ],\n",
    "    label=\"Losses\",\n",
    ")\n",
    "show_plots(plots=[{\"values\": val_auc_scores, \"label\": \"Validation AUC\"}], label=\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = OutfitDataset(data_dir, polyvore_split, split_test, transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=default_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate,\n",
    ")\n",
    "\n",
    "total_test_num_outfits = len(test_dataloader.dataset)\n",
    "print(f\"total_test_num_outfits: {total_test_num_outfits}\")\n",
    "\n",
    "test_losses = []  # to store validation losses\n",
    "test_auc_scores = []  # to store AUC scores\n",
    "test(\n",
    "    epoch=0, \n",
    "    dataloader=test_dataloader, \n",
    "    losses=test_losses, \n",
    "    auc_scores=test_auc_scores\n",
    ")\n",
    "show_plots(plots=[{\"values\": test_losses, \"label\": \"Test Losses\"}], label=\"Loss\")\n",
    "show_plots(plots=[{\"values\": test_auc_scores, \"label\": \"Test AUC\"}], label=\"AUC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outfit_recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
