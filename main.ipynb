{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "224",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\phamm\\MachineLearning_Playground\\outfit_recommendation\\main.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m split \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m normalize \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     [\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         transforms\u001b[39m.\u001b[39;49mResize(\u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         normalize,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Organizes your dataset into batches.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Batch size = number of samples processed in one iteration\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Number of batches = total samples divided by batch_size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Each this case, a sample = an outfit\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phamm/MachineLearning_Playground/outfit_recommendation/main.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_collate\u001b[39m(batch):\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\transforms\\transforms.py:348\u001b[0m, in \u001b[0;36mResize.__init__\u001b[1;34m(self, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_size \u001b[39m=\u001b[39m max_size\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(interpolation, \u001b[39mint\u001b[39m):\n\u001b[1;32m--> 348\u001b[0m     interpolation \u001b[39m=\u001b[39m _interpolation_modes_from_int(interpolation)\n\u001b[0;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpolation \u001b[39m=\u001b[39m interpolation\n\u001b[0;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mantialias \u001b[39m=\u001b[39m antialias\n",
      "File \u001b[1;32mc:\\Users\\phamm\\.conda\\envs\\outfit_recommendation\\lib\\site-packages\\torchvision\\transforms\\functional.py:48\u001b[0m, in \u001b[0;36m_interpolation_modes_from_int\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_interpolation_modes_from_int\u001b[39m(i: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InterpolationMode:\n\u001b[0;32m     40\u001b[0m     inverse_modes_mapping \u001b[39m=\u001b[39m {\n\u001b[0;32m     41\u001b[0m         \u001b[39m0\u001b[39m: InterpolationMode\u001b[39m.\u001b[39mNEAREST,\n\u001b[0;32m     42\u001b[0m         \u001b[39m2\u001b[39m: InterpolationMode\u001b[39m.\u001b[39mBILINEAR,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[39m1\u001b[39m: InterpolationMode\u001b[39m.\u001b[39mLANCZOS,\n\u001b[0;32m     47\u001b[0m     }\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m inverse_modes_mapping[i]\n",
      "\u001b[1;31mKeyError\u001b[0m: 224"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from focal_loss import FocalLoss\n",
    "from mock_dataset import MockOutfitDataset\n",
    "from outfit_model import OutfitCompatibilityModel\n",
    "from outfit_dataset import OutfitDataset\n",
    "import torch.nn as nn\n",
    "from utils import save_checkpoint\n",
    "import logging\n",
    "\n",
    "# DEBUG - INFO - WARNING - ERROR\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Should be disjoint/nondisjoint\n",
    "polyvore_split = \"nondisjoint\"\n",
    "\n",
    "# Should be traim/valid/test\n",
    "split = \"valid\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Organizes your dataset into batches.\n",
    "# Batch size = number of samples processed in one iteration\n",
    "# Number of batches = total samples divided by batch_size\n",
    "# Each this case, a sample = an outfit\n",
    "def custom_collate(batch):\n",
    "    images = torch.stack([item[\"outfit_images\"] for item in batch], dim=0)\n",
    "    texts = [item[\"outfit_texts\"] for item in batch]\n",
    "    labels = torch.stack([item[\"outfit_labels\"] for item in batch], dim=0)\n",
    "\n",
    "    return images, texts, labels\n",
    "\n",
    "\n",
    "real_dataset = OutfitDataset(data_dir, polyvore_split, split, transform)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    real_dataset, batch_size=15, shuffle=True, collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "# Instantiate the mock dataset and dataloader\n",
    "# Contains list of all outfits\n",
    "mock_dataset = MockOutfitDataset()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    mock_dataset, batch_size=15, shuffle=True, collate_fn=custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NOT USABLE\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# def show_images(images, labels):\n",
    "#     for i in range(images.size(1)):\n",
    "#         image = F.to_pil_image(images[:, i, ...])\n",
    "#         plt.subplot(1, images.size(1), i + 1)\n",
    "#         plt.imshow(image)\n",
    "#         plt.title(f\"Label: {labels[i]}\")\n",
    "#         plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# for batch_idx, (images, texts, labels) in enumerate(dataloader):\n",
    "#     print(\n",
    "#         f\"Batch {batch_idx + 1} - Shape of images: {images.shape}, Texts: {texts}, Labels: {labels}\"\n",
    "#     )\n",
    "\n",
    "#     # Visualize the images\n",
    "#     show_images(images, labels)\n",
    "    \n",
    "#     if batch_idx == 2:  # Print information for the first 3 batches\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, dataset, and dataloader\n",
    "model = OutfitCompatibilityModel()\n",
    "focal_loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        images, texts, labels = batch\n",
    "        # images = batch[\"outfit_images\"]\n",
    "        # texts = batch[\"outfit_texts\"]\n",
    "        # labels = batch[\"outfit_labels\"]\n",
    "\n",
    "        print(f\"batch - images.shape: {images.shape}\")\n",
    "        print(f\"batch - texts: {texts}\")\n",
    "        print(f\"batch - labels: {labels}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = focal_loss(\n",
    "            outputs, labels.unsqueeze(1)\n",
    "        )  # Ensure labels have the right dimension\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or log the loss if needed\n",
    "        print(f\"Epoch {epoch + 1}, Batch loss: {loss.item()}\")\n",
    "\n",
    "    save_checkpoint(model.state_dict(), \"mock\", f\"model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    # Adjust the learning rate as needed (reduce by half in steps of 10)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = param_group[\"lr\"] / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outfit_recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
